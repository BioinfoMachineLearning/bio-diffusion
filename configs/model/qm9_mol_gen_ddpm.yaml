_target_: src.models.qm9_mol_gen_ddpm.QM9MoleculeGenerationDDPM

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 1e-12
  amsgrad: true

scheduler:  # note: leaving `scheduler` empty will result in a learning-rate scheduler not being used
  # _target_: torch.optim.lr_scheduler.StepLR
  # _partial_: true
  # step_size: ${...trainer.min_epochs} // 8  # note: using literal evalution manually until Hydra natively supports this functionality
  # gamma: 0.9
  # last_epoch: -1

defaults:
  - model_cfg: qm9_mol_gen_ddpm_gcp_model.yaml
  - module_cfg: qm9_mol_gen_ddpm_gcp_module.yaml
  - layer_cfg: qm9_mol_gen_ddpm_gcp_interaction_layer.yaml
  - diffusion_cfg: qm9_mol_gen_ddpm.yaml
